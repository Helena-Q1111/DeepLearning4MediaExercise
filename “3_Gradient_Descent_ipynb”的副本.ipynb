{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Helena-Q1111/DeepLearning4MediaExercise/blob/main/%E2%80%9C3_Gradient_Descent_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for Media\n",
        "#### MPATE-GE 2039 - DM-GY 9103\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is a class excercise, it counts towards your class participation grade.\n",
        "\n",
        "This notebook is a playground to understand gradient descent."
      ],
      "metadata": {
        "id": "PgYHg43oYOk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipympl"
      ],
      "metadata": {
        "id": "bL6wh_-Pht9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmIZX_xBtrjy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "C-2NBGhei5RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define some functions and their rate of change"
      ],
      "metadata": {
        "id": "52f92dJG3c-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function (e.g. a cuadratic function, cosine)\n",
        "l = lambda y: (y-1)**2+1 # quadratic function\n",
        "# f = np.cos # cosine function, uncomment to try\n",
        "\n",
        "\n",
        "# define the support of the function\n",
        "wmin = -5.0\n",
        "wmax = 5\n",
        "nsteps = 200\n",
        "# the function will be evaluated in nsteps equally spaced from xmin to xmax\n",
        "w = np.linspace(wmin, wmax, nsteps)"
      ],
      "metadata": {
        "id": "sH_vXS7ytt2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the `rate_of_change`, which is an approximation of the derivative of a function when $\\epsilon$ is very small."
      ],
      "metadata": {
        "id": "KK61cYLQZFcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the rate of change. Note that it is independent of the type\n",
        "# function we are using (e.g. cosine or quadratic).\n",
        "eps = 10**-8\n",
        "rate_change = lambda y: (l(y+eps) - l(y))/eps"
      ],
      "metadata": {
        "id": "sxeJteqRt0TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's observe the function and the rate of change together. We should observe that:\n",
        "\n",
        "*   Where the function goes \"downhill\", the rate of change is negative.\n",
        "*   Where there is a plateau (minimum) or hill (maximum) the rate of change is 0.\n",
        "*   If the function goes \"uphill\", the rate of change is positive.\n"
      ],
      "metadata": {
        "id": "IGeSjldYaCqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.clear_output()\n",
        "\n",
        "w0 = -1 # a random point\n",
        "\n",
        "fig = plt.figure(figsize=(10,5), facecolor='white')\n",
        "plt.plot(w, l(w), label = \"l(w)\")\n",
        "plt.plot(w, rate_change(w), label = \"rate_change(w)\")\n",
        "plt.hlines(0, np.min(w), np.max(w), label='zero', color='black', linestyle='--')\n",
        "plt.scatter(w0, l(w0), color='red', label='l(w0)', marker='o')\n",
        "plt.legend()\n",
        "plt.title('l(w) vs. rate of change')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S3CdJoazuDmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to use the information from the rate of change to move from a random starting point ($x_0$) to the minimum value of the function. In other words: How should we change $x_0$ so it lands in the lowest point of $f(x)$?\n",
        "Should we move forward or backwards?\n",
        "i.e. should we increment the value of $x_0$ or decrease it?\n",
        "\n",
        "\n",
        "```\n",
        "If we change x_0 in the opposite direction as the derivative (e.g. increase x0 when the derivative is negative),\n",
        "we will always move towards lower values of f(x).\n",
        "```\n",
        "\n",
        "We can do that by substracting $rc$ to $x_0$, times a small `step_size` constant. Let's look at that.\n"
      ],
      "metadata": {
        "id": "wk15BDpckkbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a naive Gradient Descent algorithm"
      ],
      "metadata": {
        "id": "0bElLzTP3ii8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_gd(lr=0.1, nsteps=50, rate_change=rate_change, w0=None):\n",
        "    '''\n",
        "    Naive implementation of a Gradient Descent (GD) algorithm.\n",
        "\n",
        "    lr (float): learning rate, determines how big is the step taken by GD.\n",
        "    nsteps (int): number of iterations to run GD.\n",
        "    rate_change (np.ufunc): finite difference approximation of the function.\n",
        "    w0 (float): starting point for the algorightm. If None, a random number\n",
        "                is uniformely sampled between -2 and 2.\n",
        "\n",
        "    Returns:\n",
        "    w_list (list of float): positions in the x axis after applying GD.\n",
        "    '''\n",
        "    # initialize iterative process\n",
        "    w_iter = w0\n",
        "    if not w_iter:\n",
        "      w_iter = np.random.uniform(-2,2,1)[0]\n",
        "\n",
        "    # at each step, substract the derivative * the learning rate\n",
        "    w_list = [w_iter]\n",
        "    for i in range(0,nsteps):\n",
        "      w_iter = w_iter - lr* rate_change(w_iter)\n",
        "      w_list.append(w_iter)\n",
        "\n",
        "    return np.array(w_list)\n"
      ],
      "metadata": {
        "id": "G11hrRlauFNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.clear_output()\n",
        "# obtain the list of positions for gradient descent\n",
        "w_gd = naive_gd(w0=w0)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5), facecolor='white')\n",
        "plt.plot(l(w_gd), label='l(w_gd)')\n",
        "plt.scatter(0, l(w0), color='red', label='l(w0)', marker='o')\n",
        "plt.legend()\n",
        "plt.title(\"Values of the function evaluated in the points from GD\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nxZAmbwLwWHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the algorithm was successful, we should see a decreasing function, where the highest value was the starting point ($x_0$). Let's see how that looks like along with the function, where $x_0$ is the starting point and $x_niter$ is the last one."
      ],
      "metadata": {
        "id": "jDrNr3dqv9rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.clear_output()\n",
        "\n",
        "fig = plt.figure(figsize=(10,5), facecolor='white')\n",
        "\n",
        "plt.scatter(w_gd, l(w_gd), color='black', label='gradient descent')\n",
        "w_big = np.linspace(np.minimum(np.min(w_gd), wmin),\n",
        "                    np.maximum(np.max(w_gd), wmax),\n",
        "                    200)\n",
        "plt.scatter(w0, l(w0), color='red', label='l(w0)', marker='o')\n",
        "plt.scatter(w_gd[-1], l(w_gd[-1]), color='green', label='l(w_niter)', marker='o')\n",
        "\n",
        "plt.plot(w_big, l(w_big), label = \"l(w)\")\n",
        "plt.plot(w_big, rate_change(w_big), label = \"rate_change(w)\" )\n",
        "plt.title(\"GD iterations\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wgO8Se5AwdQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer some questions"
      ],
      "metadata": {
        "id": "recAGSlN3sz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to answer a few questions:\n",
        "\n",
        "1. Why do you think that the algorithm takes \"bigger steps\" at the beginning? i.e. closer to the starting point? Is this always the case?\n",
        "2. How does the `learning rate` affect the algorithm, e.g. if you increase and decrease its value? What is it doing?\n",
        "3. Can you break any of the examples, i.e. so the algorithm does not find the minimum? How? What happened?\n",
        "4. What is the effect of the number of iterations?\n",
        "5. What happens if the rate of change is zero? What doews it mean?"
      ],
      "metadata": {
        "id": "qrexmdbXw8aw"
      }
    }
  ]
}